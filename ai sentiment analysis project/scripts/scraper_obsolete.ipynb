{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cce64fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: HTTP 403\n",
      "No data was scraped. Check API or subreddit name.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def fetch_posts(subreddit, limit=100, before=None):\n",
    "    url = \"https://api.pushshift.io/reddit/search/submission\"\n",
    "    params = {\n",
    "        \"subreddit\": subreddit,\n",
    "        \"size\": limit,\n",
    "        \"before\": before,\n",
    "        \"selftext:not\": \"[removed]\",\n",
    "        \"sort\": \"desc\"\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error: HTTP {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "    json_data = response.json()\n",
    "    if \"data\" not in json_data:\n",
    "        print(\"No data found in response.\")\n",
    "        return []\n",
    "\n",
    "    return json_data[\"data\"]\n",
    "\n",
    "def scrape_subreddit(subreddit, max_posts=500):\n",
    "    all_posts = []\n",
    "    before = int(time.time())\n",
    "\n",
    "    while len(all_posts) < max_posts:\n",
    "        batch = fetch_posts(subreddit, limit=100, before=before)\n",
    "        if not batch:\n",
    "            break\n",
    "        all_posts.extend(batch)\n",
    "        before = batch[-1][\"created_utc\"]\n",
    "        time.sleep(1)\n",
    "\n",
    "    return pd.DataFrame([\n",
    "        {\n",
    "            'timestamp': post['created_utc'],\n",
    "            'title': post.get('title', ''),\n",
    "            'selftext': post.get('selftext', ''),\n",
    "            'subreddit': post.get('subreddit', ''),\n",
    "            'score': post.get('score', 0)\n",
    "        } for post in all_posts\n",
    "    ])\n",
    "    \n",
    "\n",
    "df = scrape_subreddit('depression', max_posts=500)\n",
    "\n",
    "if df is not None and not df.empty:\n",
    "    df.to_csv('depression_posts.csv', index=False)\n",
    "    print(\"Saved CSV with\", len(df), \"posts.\")\n",
    "else:\n",
    "    print(\"No data was scraped. Check API or subreddit name.\")\n",
    "\n",
    "df.to_csv('../data/depression_posts.csv', index= False)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
